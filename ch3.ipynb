{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ff26d1",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4d97b",
   "metadata": {},
   "source": [
    "### 3.3 Attending to different parts of the input with self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b037f",
   "metadata": {},
   "source": [
    "#### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b71df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider that z^2 is the input vector x^2 but with information about all the other input\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step     (x^6)\n",
    "    ]\n",
    ") # rows are words, columns are embedding dimensions\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c46d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we use the second input as query => q^2 = x^2\n",
    "# The attention scores are: w21 = x^1 @ q^2.T; w22 = x^2 @ q^2.T; ... (w = omega)\n",
    "# The attention weights are then these values normalized via softmax\n",
    "query_2 = inputs[1]\n",
    "score_dim = inputs.shape[0]\n",
    "attn_scores_2 = torch.zeros(score_dim)\n",
    "for i, input_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(input_i, query_2) # Multiplying each input with input_2 to get the attention scores for input_2\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "038189ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ee2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is equivalent to a matrix multiplication:\n",
    "assert torch.equal(attn_scores_2, inputs @ query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60f2b59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Normalize the scores so they sum up to 1 (they are thus called \"weights\")\n",
    "# This is done to ease gradient learning, avoiding too large/small values\n",
    "# Softmax does this best, by accounting for outliers\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0) # it's a just one row anyway \n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a41e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fef74efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: compute the context vector z_2 by multiplying the inputs by their respective weights\n",
    "# Which is again just a matrix dot product\n",
    "context_vec_2 = attn_weights_2 @ inputs\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753500b",
   "metadata": {},
   "source": [
    "#### 3.3.2 Computing attention weights for ALL input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11d0b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_all = torch.empty(6,6)\n",
    "\n",
    "# Apply previous Step 1 to all pairwise elements to compute the unnormalized attention score matrix\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores_all[i, j] = x_i @ x_j\n",
    "\n",
    "# Equivalent to:\n",
    "# attn_scores_all = inputs @ inputs.T\n",
    "\n",
    "print(attn_scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af850bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "\n",
      "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print()\n",
    "print(inputs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d73b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: normalize\n",
    "attn_weights_all = torch.softmax(attn_scores_all, dim=1)\n",
    "print(attn_weights_all)\n",
    "attn_weights_all.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70e1bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get all context vectors z_i\n",
    "context_vec_all = attn_weights_all @ inputs\n",
    "print(context_vec_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80412c7e",
   "metadata": {},
   "source": [
    "These are now the same input tokens, modified to take context (surrounding words) into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241a871",
   "metadata": {},
   "source": [
    "# 3.4 Implementing self-attention with trainable weights (_fucked_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b828cd",
   "metadata": {},
   "source": [
    "A.k.a _scaled dot-product attention_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafc28f",
   "metadata": {},
   "source": [
    "####  3.4.1 Implementing the weights step by step\n",
    "We start by calculating the context vector for just one input element, x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e6ed0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1] # input's embedding size, d=3\n",
    "d_out = 2 # output's embedding size, d=2\n",
    "# usually d_in==d_out, but we change this for better illustration\n",
    "\n",
    "# Set the query, key, and value weight matrices\n",
    "# The purpose of these weight matrices is to project each input into things that are trainable and usable in different scenarios (when input is considered the query, for example) \n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # This should be True, but we turn it off to reduce output clutter\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Get q,k,v vecs\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value \n",
    "\n",
    "print(query_2) # 2d, since d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74c4ccd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even though we're only trying to get z_2, we still need all keys and values (for the other inputs)\n",
    "# because they're involved in computing the attention weights wrt q_2\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96a96237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute the attention scores\n",
    "# To do this, you multiply the given query by all the keys\n",
    "attn_scores_2 = keys @ query_2 # same as doing query2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ebbbdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Getting attetion weights via scaled softmax\n",
    "d_k = keys.shape[-1] # 2\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / (d_k**0.5), dim=-1) # To prevent grads from getting too large and the softmax function from acting like a step function\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bae49e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Context vec for x_2\n",
    "context_vec_2 = attn_weights_2 @ values #* VALUES!\n",
    "z_2 = context_vec_2\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb08671",
   "metadata": {},
   "source": [
    "#### 3.4.2 Implementing a Python class for the self-attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d4a15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import softmax\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ W_query\n",
    "        keys = x @ W_key\n",
    "        values = x @ W_value\n",
    "        attn_scores = queries @ keys.T # both are 6,2 with example input\n",
    "        attn_weights = softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1) # omega, or w\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9820704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa = SelfAttention_v1(d_in, d_out)\n",
    "print(sa(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7733f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model by using Linear instead of Parameter, which\n",
    "# has an optimized weight initialization scheme and, when bias=False,\n",
    "# effectively does a matrix multiplication\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, kqv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ W_query\n",
    "        keys = x @ W_key\n",
    "        values = x @ W_value\n",
    "        attn_scores = queries @ keys.T  # both are 6,2 with example input\n",
    "        attn_weights = softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)  # omega, or w\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "491ddfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
