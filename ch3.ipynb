{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ff26d1",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4d97b",
   "metadata": {},
   "source": [
    "### 3.3 Attending to different parts of the input with self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b037f",
   "metadata": {},
   "source": [
    "#### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23b71df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider that z^2 is the input vector x^2 but with information about all the other input\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your     (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey  (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts   (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with     (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one      (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step     (x^6)\n",
    "    ]\n",
    ") # rows are words, columns are embedding dimensions\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c46d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Suppose we use the second input as query => q^2 = x^2\n",
    "# The attention scores are: w21 = x^1 @ q^2.T; w22 = x^2 @ q^2.T; ... (w = omega)\n",
    "# The attention weights are then these values normalized via softmax\n",
    "query_2 = inputs[1]\n",
    "score_dim = inputs.shape[0]\n",
    "attn_scores_2 = torch.zeros(score_dim)\n",
    "for i, input_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(input_i, query_2) # Multiplying each input with input_2 to get the attention scores for input_2\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "038189ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0ee2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is equivalent to a matrix multiplication:\n",
    "assert torch.equal(attn_scores_2, inputs @ query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60f2b59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Normalize the scores so they sum up to 1 (they are thus called \"weights\")\n",
    "# This is done to ease gradient learning, avoiding too large/small values\n",
    "# Softmax does this best, by accounting for outliers\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0) # it's a just one row anyway \n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47a41e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(attn_weights_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fef74efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: compute the context vector z_2 by multiplying the inputs by their respective weights\n",
    "# Which is again just a matrix dot product\n",
    "context_vec_2 = attn_weights_2 @ inputs\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753500b",
   "metadata": {},
   "source": [
    "#### 3.3.2 Computing attention weights for ALL input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11d0b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_all = torch.empty(6,6)\n",
    "\n",
    "# Apply previous Step 1 to all pairwise elements to compute the unnormalized attention score matrix\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores_all[i, j] = x_i @ x_j\n",
    "\n",
    "# Equivalent to:\n",
    "# attn_scores_all = inputs @ inputs.T\n",
    "\n",
    "print(attn_scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af850bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "\n",
      "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print()\n",
    "print(inputs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d73b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: normalize\n",
    "attn_weights_all = torch.softmax(attn_scores_all, dim=1)\n",
    "print(attn_weights_all)\n",
    "attn_weights_all.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70e1bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get all context vectors z_i\n",
    "context_vec_all = attn_weights_all @ inputs\n",
    "print(context_vec_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80412c7e",
   "metadata": {},
   "source": [
    "These are now the same input tokens, modified to take context (surrounding words) into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241a871",
   "metadata": {},
   "source": [
    "# 3.4 Implementing self-attention with trainable weights (_fucked_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b828cd",
   "metadata": {},
   "source": [
    "A.k.a _scaled dot-product attention_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafc28f",
   "metadata": {},
   "source": [
    "####  3.4.1 Implementing the weights step by step\n",
    "We start by calculating the context vector for just one input element, x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e6ed0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1] # input's embedding size, d=3\n",
    "d_out = 2 # output's embedding size, d=2\n",
    "# usually d_in==d_out, but we change this for better illustration\n",
    "\n",
    "# Set the query, key, and value weight matrices\n",
    "# The purpose of these weight matrices is to project each input into things that are trainable and usable in different scenarios (when input is considered the query, for example) \n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # This should be True, but we turn it off to reduce output clutter\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Get q,k,v vecs\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value \n",
    "\n",
    "print(query_2) # 2d, since d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74c4ccd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even though we're only trying to get z_2, we still need all keys and values (for the other inputs)\n",
    "# because they're involved in computing the attention weights wrt q_2\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96a96237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute the attention scores\n",
    "# To do this, you multiply the given query by all the keys\n",
    "attn_scores_2 = keys @ query_2 # same as doing query2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ebbbdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Getting attetion weights via scaled softmax\n",
    "d_k = keys.shape[-1] # 2\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / (d_k**0.5), dim=-1) # To prevent grads from getting too large and the softmax function from acting like a step function\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bae49e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Context vec for x_2\n",
    "context_vec_2 = attn_weights_2 @ values #* VALUES!\n",
    "z_2 = context_vec_2\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb08671",
   "metadata": {},
   "source": [
    "#### 3.4.2 Implementing a Python class for the self-attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d4a15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import softmax\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ W_query\n",
    "        keys = x @ W_key\n",
    "        values = x @ W_value\n",
    "        attn_scores = queries @ keys.T # both are 6,2 with example input\n",
    "        attn_weights = softmax(attn_scores / (keys.shape[-1]**0.5), dim=-1) # omega, or w\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9820704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa = SelfAttention_v1(d_in, d_out)\n",
    "print(sa(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7733f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model by using Linear instead of Parameter, which\n",
    "# has an optimized weight initialization scheme and, when bias=False,\n",
    "# effectively does a matrix multiplication\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, kqv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=kqv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ W_query\n",
    "        keys = x @ W_key\n",
    "        values = x @ W_value\n",
    "        attn_scores = queries @ keys.T  # both are 6,2 with example input\n",
    "        attn_weights = softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)  # omega, or w\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "491ddfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a07c05",
   "metadata": {},
   "source": [
    "### 3.5 Implementing causal/masked attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e437b3c",
   "metadata": {},
   "source": [
    "#### 3.5.1 Causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7df7c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Getting the weights so we can mask out the upper diagonal and normalize\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = softmax(attn_scores / (keys.shape[-1]  ** 0.5), dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6aca69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Creating a simple mask by setting the lower diagonal\n",
    "diagonal = torch.tril(torch.ones_like(attn_weights))\n",
    "masked_simple = attn_weights * diagonal\n",
    "print(masked_simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60aa3e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalize the weights using the row sums\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e70b54",
   "metadata": {},
   "source": [
    "We can do this whole thing faster by inf'ing out the top diagonal of the attn_scores and taking the softmax of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7a69a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones_like(attn_weights), diagonal=1)\n",
    "masked_smart = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23f62411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = softmax(masked_smart / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)\n",
    "print(attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd10277",
   "metadata": {},
   "source": [
    "#### 3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40b5ffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # drop half, p=.5\n",
    "example = torch.ones(6,6)\n",
    "dropout(example) # notice that the values are rescaled by 1/p. This is to ensure that average effect of attention stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3975d8",
   "metadata": {},
   "source": [
    "#### 3.5.3 Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05e75c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "\n",
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "# first, create a batched example so that it works with prev created class\n",
    "batch = torch.stack((inputs, inputs))\n",
    "print(batch.shape)\n",
    "print()\n",
    "print(inputs)\n",
    "print()\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "385343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)         \n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)         \n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2) # switch dims 1 & 2\n",
    "        attn_scores_masked = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # in-place cuz of \"_\" \n",
    "        attn_weights = softmax(attn_scores_masked / keys.shape[-1]**.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vecs = attn_weights @ values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d917ae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae89572",
   "metadata": {},
   "source": [
    "# 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bbd4a",
   "metadata": {},
   "source": [
    "## 3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35ff74c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4])\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we start with a simple sequential version, and then we make it parallel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias=qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]  # # of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, 2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs.shape) # last dim=4 because d_out*2=2*2 (two heads). Remember that the two heads stack the vectors by column/vertically.\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309458f8",
   "metadata": {},
   "source": [
    "## 3.6.2 Implementing multi-head attention with weight splits (parallel version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42f5022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n",
    "        # this will result in errors in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # fine because loop-order stays the same, just unrolled\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now transpose so that we have token-major order\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (self-attention) w/ causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # Need to call contiguous() because view expects data to be that way, and transpose changes it\n",
    "        context_vec = self.out_proj(context_vec) # optional projection (used in gpt2 and most others)\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79fecb",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). Note that the smallest GPT-2 model supports a context length of 1,024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71b1e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (W_key): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (W_value): Linear(in_features=3, out_features=2, bias=False)\n",
      "  (out_proj): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mha_big = MultiHeadAttention(768, 768, 1024, .5, 12)\n",
    "print(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1aec3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([2, 768, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand(768, 768)\n",
    "print(inputs.shape)\n",
    "batch = torch.stack((inputs, inputs))\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fd794931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 ms ± 3.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "542 μs ± 142 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# cpu\n",
    "%timeit mha_big(batch)\n",
    "\n",
    "# gpu\n",
    "batch_gpu = batch.to(\"cuda\")\n",
    "mha_big_gpu = MultiHeadAttention(768, 768, 1024, .5, 12).to(\"cuda\")\n",
    "%timeit mha_big_gpu(batch_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
